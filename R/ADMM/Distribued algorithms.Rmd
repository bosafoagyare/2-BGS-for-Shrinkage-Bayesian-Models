---
title: "Distributed/ Divide & Conquer"
output:
  pdf_document: default
  html_document: default
date: "2023-04-03"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# (1)--------------- Using ADMM for KRR ------

## Approach
Distributed ADMM (Alternating Direction Method of Multipliers) can be used to solve Kernel Ridge Regression for large scale datasets that cannot be processed on a single machine. The basic idea of distributed ADMM is to split the dataset across multiple machines, perform local updates on each machine, and then communicate the updates across the machines to obtain a global solution. Here's an implementation of distributed ADMM for Kernel Ridge Regression in R.


### Math

```{r, eval=FALSE}
library(parallel)
library(kernlab)

# Generate some sample data
set.seed(123)
n <- 1000
p <- 100
X <- matrix(rnorm(n*p), n, p)
y <- rnorm(n)


# Define the kernel function
kern   <- rbfdot(sigma=1/p)
k      <- kernelMatrix(kern, X)

# Define the distributed ADMM algorithm
distributed_admm <- function(X, y, k, lambda, rho, num_iterations, num_workers) {
  n <- nrow(X)
  p <- ncol(X)
  
  # Split the dataset into equal-sized chunks
  chunk_size <- ceiling(n / num_workers)
  chunks <- split(1:n, rep(1:num_workers, each = chunk_size)[1:n])
  
  # Initialize variables
  alpha <- matrix(0, n, 1)
  u <- matrix(0, n, num_workers)
  z <- matrix(0, n, num_workers)
  
  # Compute the inverse of the local kernel matrices
  inv_k <- lapply(chunks, function(idx) solve(k[idx,idx] + (rho/2) * diag(chunk_size)))
  
  # Define the local updates function
  local_update <- function(chunk_idx, chunk, alpha, u, z) {
    # Compute the local residual and update u
    r <- y[chunk_idx] - k[chunk_idx,] %*% alpha - z[chunk_idx,] + u[chunk_idx,]
    u[chunk_idx,] <- u[chunk_idx,] + r
    
    # Update alpha
    alpha[chunk_idx,] <- inv_k[[chunk_idx]] %*% (k[chunk_idx,] %*% alpha + r - z[chunk_idx,] + u[chunk_idx,])
    
    # Update z
    z[chunk_idx,] <- (rho / (rho + 1)) * (k[chunk_idx,] %*% alpha + u[chunk_idx,])
    
    return(list(alpha = alpha, u = u, z = z))
  }
  
  # Run the distributed ADMM algorithm
  for (iter in 1:num_iterations) {
    # Parallelize the local updates across workers
    local_results <- mclapply(seq_along(chunks), function(i) local_update(chunks[[i]], X[chunks[[i]],], alpha, u, z), mc.cores = num_workers)
    
    # Combine the local updates
    alpha <- Reduce(`+`, lapply(local_results, function(result) result$alpha))
    u <- Reduce(`+`, lapply(local_results, function(result) result$u))
    z <- Reduce(`+`, lapply(local_results, function(result) result$z))
    
    # Apply the global consensus step
    alpha <- alpha / num_workers
    u <- u / num_workers
    z <- z / num_workers
  }
  
  return(alpha)
}

# Run the distributed ADMM algorithm with 4 workers, lambda = 0.01, rho = 1, and 10 iterations
num_workers <- 4
lambda <- 0.01
rho <- 1
num_iterations <- 100
alpha <- distributed_admm(X, y, k, lambda, rho, num_iterations, num_workers)

# Compute the predictions and mean squared error
y_pred <- k %*% alpha
mse <- mean((y - y_pred)^2)
print(paste("KRR ADMM MSE:", mse))

```

> In this implementation, we split the dataset into equal-sized chunks and perform local updates on each chunk using the kernel matrix and its inverse. We then combine the local updates by averaging them, and apply a global consensus step to obtain the next iteration. The process is repeated for a fixed number of iterations. Finally, we compute the predictions and mean squared error using the obtained alpha values.










```{r}
#---------- Serial KRR
library(kernlab)

# Generate some sample data
set.seed(123)
n <- 1000
p <- 100
X <- matrix(rnorm(n*p), n, p)
y <- rnorm(n)
lambda <- 0.01


# Define the kernel function
kern   <- rbfdot(sigma=1/p)
k      <- kernelMatrix(kern, X)

krr <- function(y, k) {
  n = nrow(k)
  alpha <- solve(k + lambda * diag(n), y)
  return(alpha)
}

krr_alpha <- krr(y, k)
y_krr_pre <- k %*% krr_alpha
mse <- mean((y - y_krr_pre)^2)
print(paste("KRR MSE:", mse))
```





```{r}
#----- Mapreduce
# Define the Gaussian kernel function
gaussian_kernel <- function(x, y, sigma) {
  -exp(-sum((x - y)^2) / (2 * sigma^2))
}

# Define the Map function for KRR
krr_map <- function(key, value) {
  X_i <- value$X
  y_i <- value$y
  K_i <- matrix(0, nrow(X_i), nrow(X_i))
  for (j in 1:nrow(X_i)) {
    for (k in 1:nrow(X_i)) {
      K_i[j, k] <- gaussian_kernel(X_i[j, ], X_i[k, ], sigma)
    }
  }
  K_i <- K_i + lambda * nrow(X_i) * diag(nrow(X_i))
  K_i_inv <- solve(K_i)
  w_i <- K_i_inv %*% t(X_i) %*% y_i
  emit(key, w_i)
}

# Define the Reduce function for KRR
krr_reduce <- function(key, values) {
  n <- length(values)
  w <- matrix(0, ncol(values[[1]]), 1)
  for (i in 1:n) {
    w <- w + values[[i]]
  }
  w <- w / n
  emit(key, w)
}

# Define the MapReduce function for KRR
mapreduce_krr <- function(X, y, lambda, sigma, num_reducers) {
  # Partition the data into num_reducers subsets
  n <- nrow(X)
  p <- ncol(X)
  subset_size <- ceiling(n / num_reducers)
  subsets <- list()
  for (i in 1:num_reducers) {
    start <- (i - 1) * subset_size + 1
    end <- min(i * subset_size, n)
    X_i <- X[start:end, ]
    y_i <- y[start:end, ]
    subsets[[i]] <- list(X = X_i, y = y_i)
  }
  
  # Run the MapReduce algorithm for KRR
  w <- matrix(0, p, 1)
  for (i in 1:num_reducers) {
    map_results <- map(subsets[i], krr_map)
    reduce_results <- reduce(map_results, krr_reduce, num_reducers)
    w <- w + reduce_results[[1]]
  }
  w <- w / num_reducers
  
  # Return the weight vector
  return(w)
}


# Generate some random data
n <- 1000
p <- 10
X <- matrix(rnorm(n * p), nrow = n)
y <- matrix(rnorm(n), nrow = n)

# Run the MapReduce KRR algorithm
lambda <- 1
sigma <- 1
num_reducers <- 4
w <- mapreduce_krr(X, y, lambda, sigma, num_reducers)

```



